{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ea4daf",
   "metadata": {},
   "source": [
    "### **ASISTENTE VIRTUAL INTERACTIVO PARA TU MOTO BOXER CT100KS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd382b",
   "metadata": {},
   "source": [
    "## Importacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b007ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\miniconda3\\envs\\proyecto_rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import rank_bm25\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "import pdfplumber\n",
    "from langchain.schema import Document\n",
    "import pyttsx3\n",
    "import logging\n",
    "from voz_text import query_voz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c275d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "api_key_groq = os.getenv(\"GROQ_API_KEY\")  \n",
    "\n",
    "# Aseg√∫rate de tener .env con GEMINI_API_KEY\n",
    "\n",
    "# Directorio con PDFs y archivo de historial\n",
    "DOCS_DIR = \"./data/\"\n",
    "HISTORY_FILE = \"./chat_history.json\"\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "\n",
    "# Text Processing\n",
    "CHUNK_SIZE = 1000  \n",
    "CHUNK_OVERLAP = 100  \n",
    "\n",
    "def limpiar_consola():\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb3817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar y chunkear PDFs\n",
    "def load_and_chunk_pdfs():\n",
    "    documents = []\n",
    "    for filename in os.listdir(DOCS_DIR):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(DOCS_DIR, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    # Extraer texto\n",
    "                    text = page.extract_text()\n",
    "                    \n",
    "                    # Extraer tablas\n",
    "                    tables = page.extract_tables()\n",
    "                    table_text = \"\"\n",
    "                    for table in tables:\n",
    "                        for row in table:\n",
    "                            # Filtrar valores None y convertir a string\n",
    "                            row_text = \" | \".join(str(cell) if cell is not None else \"\" for cell in row)\n",
    "                            table_text += row_text + \"\\n\"\n",
    "                    \n",
    "                    # Combinar texto y tablas\n",
    "                    combined_text = f\"{text}\\n\\nTablas:\\n{table_text}\"\n",
    "                    \n",
    "                    # Crear documento con metadatos\n",
    "                    doc = Document(\n",
    "                        page_content=combined_text,\n",
    "                        metadata={\n",
    "                            \"source\": filename,\n",
    "                            \"page\": page_num + 1\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "    \n",
    "    # Dividir documentos en fragmentos\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae90f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_document_samples(chunks):\n",
    "    print(\"\\nMuestras de documentos cargados:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):  # Mostrar primeros 3 chunks\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"Fuente: {chunk.metadata.get('source')}\")\n",
    "        print(f\"P√°gina: {chunk.metadata.get('page')}\")\n",
    "        print(\"Contenido:\")\n",
    "        print(chunk.page_content[:200] + \"...\")  # Primeros 200 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed27b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Crear o cargar base de datos vectorial\n",
    "def create_or_load_vector_store(chunks, persist_directory=PERSIST_DIR, force_reload=False, embedding=\"\"):\n",
    "    if force_reload and os.path.exists(persist_directory):\n",
    "        import shutil\n",
    "        shutil.rmtree(persist_directory)\n",
    "\n",
    "    if os.path.exists(persist_directory) and os.listdir(persist_directory) and not force_reload:\n",
    "        vector_store = Chroma(\n",
    "            collection_name=\"bajaj_boxer\",\n",
    "            embedding_function=HuggingFaceEmbeddings(model_name=embedding),\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "    else:\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=HuggingFaceEmbeddings(model_name=embedding),\n",
    "            collection_name=\"bajaj_boxer\",\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98913022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Cargar o inicializar historial\n",
    "def load_chat_history():\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "    return []\n",
    "\n",
    "def save_chat_history(history):\n",
    "    with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history[-5:], f, ensure_ascii=False, indent=2)  # Limitar a las √∫ltimas 10 interacciones\n",
    "\n",
    "def save_ragas_history(question, answer, contexts, filename=\"ragas/ragas_history_E2L2.json\"):\n",
    "    \"\"\"\n",
    "    Guarda la pregunta, respuesta generada, contexto recuperado y un campo ground_truth vac√≠o en un archivo JSON para evaluaci√≥n con RAGAS.\n",
    "    \"\"\"\n",
    "    entry = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": \"\"\n",
    "    }\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = []\n",
    "    data.append(entry)\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce43fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Configurar el sistema RAG con memoria\n",
    "def setup_rag(vector_store, model, chunks):\n",
    "    if model ==\"1\":\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            google_api_key=api_key,\n",
    "            temperature=0\n",
    "        )\n",
    "    elif model == \"2\":\n",
    "        llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            api_key=api_key_groq,\n",
    "            temperature=0\n",
    "        )\n",
    "    elif model == \"3\":\n",
    "        llm = ChatGroq(\n",
    "            model=\"gemma2-9b-it\",\n",
    "            api_key=api_key_groq,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\"\n",
    "    )\n",
    "    \n",
    "    # Prompt personalizado para tono natural y cordial\n",
    "    prompt_template = \"\"\"\n",
    "    Eres un asistente experto en motocicletas Bajaj Boxer CT100 KS. \n",
    "    Tu trabajo es ayudar a los usuarios con informaci√≥n t√©cnica, mantenimiento y operaci√≥n de esta motocicleta espec√≠fica.\n",
    "\n",
    "    HISTORIAL DE LA CONVERSACI√ìN:\n",
    "    {chat_history}\n",
    "\n",
    "    CONTEXTO RELEVANTE DE LA DOCUMENTACI√ìN:\n",
    "    {context}\n",
    "\n",
    "    PREGUNTA DEL USUARIO: {question}\n",
    "\n",
    "    INSTRUCCIONES PARA TU RESPUESTA:\n",
    "    - Responde √öNICAMENTE sobre la Bajaj Boxer CT100 KS y si hablaras de otra moto, especificalo\n",
    "    - Usa un tono cordial, natural y profesional en espa√±ol\n",
    "    - Si el usuario te pregunta sobre otra moto, especificalo\n",
    "    - No te extiendas en tu respuesta, a no ser que el usuario te lo pida y osea necesario\n",
    "    - Basa tu respuesta en la informaci√≥n del contexto proporcionado y el historial de la conversaci√≥n\n",
    "    - Si la informaci√≥n est√° en una tabla, incluye los valores espec√≠ficos\n",
    "    - Si la informaci√≥n no est√° en el contexto, dilo claramente\n",
    "    - Proporciona respuestas pr√°cticas y √∫tiles para el usuario\n",
    "    - Incluye detalles t√©cnicos relevantes cuando sea apropiado\n",
    "    - Si mencionas especificaciones t√©cnicas, cita los valores exactos\n",
    "    - Mant√©n la respuesta completa pero concisa, no la hagas tan extensa\n",
    "\n",
    "    RESPUESTA:\n",
    "    \"\"\" \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"context\", \"question\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    '''bm25 = inicializar_Bm25(chunks)\n",
    "    inicializar_chroma = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    retriever = combine_serch(retrievers=[bm25, inicializar_chroma], weights=[0.5, 0.5])'''\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0062842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decir_respuesta(texto):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty('rate', 200)\n",
    "    engine.setProperty('volume', 1)\n",
    "    os.makedirs(\"audio\", exist_ok=True)\n",
    "    engine.say(texto)\n",
    "    engine.save_to_file(texto, \"audio/answer_ia_voz.mp3\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "def responder_desde_rag(pregunta, vector_store, rag_chain):\n",
    "    result = rag_chain.invoke({\"question\": pregunta})\n",
    "    respuesta = result[\"answer\"]\n",
    "\n",
    "    contexts = \"\\n\\n\".join([doc.page_content for doc in result.get(\"source_documents\", [])])\n",
    "    save_ragas_history(pregunta, respuesta, contexts)\n",
    "    decir_respuesta(respuesta)  # genera el MP3\n",
    "\n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Ciclo interactivo en consola\n",
    "def main():\n",
    "    print(\"¬øQuieres (C)argar la base vectorial existente o (R)ecrearla con los documentos actuales?\")\n",
    "    op = input(\"Escribe C para cargar o R para recrear: \").strip().lower()\n",
    "    op2 = input(\"Que tipo de embeddings quieres usar? (1) intfloat/multilingual-e5-base (2) sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2:\\n \")\n",
    "    force_reload = (op == \"r\")\n",
    "\n",
    "    if op2 == \"1\":\n",
    "        embeddings = \"intfloat/multilingual-e5-base\"\n",
    "    elif op2 == \"2\":\n",
    "        embeddings = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    else:\n",
    "        print(\"Opci√≥n no v√°lida\")\n",
    "    \n",
    "    model = input(\"Que modelo de IA quieres usar? (1) Gemini-2.0-flash (2) Llama-3.3-70b-versatile: (3) gemma2-9b-it: \\n\")\n",
    "\n",
    "    if force_reload:\n",
    "        print(\"Cargando y procesando documentos...\")\n",
    "        chunks = load_and_chunk_pdfs()\n",
    "        print(f\"Se cargaron {len(chunks)} fragmentos de documentos.\")\n",
    "        print_document_samples(chunks)\n",
    "    else:\n",
    "        chunks = []\n",
    "\n",
    "    if embeddings == \"intfloat/multilingual-e5-base\":\n",
    "        PERSIST_DIR = \"./chroma_db_intfloat\"\n",
    "    elif embeddings == \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\":\n",
    "        PERSIST_DIR = \"./chroma_db_paraphrase\"\n",
    "\n",
    "    print(\"Cargando base de datos vectorial...\")\n",
    "    vector_store = create_or_load_vector_store(chunks, PERSIST_DIR, force_reload=force_reload, embedding=embeddings)\n",
    "    print(\"Configurando sistema RAG con memoria...\")\n",
    "\n",
    "    rag_chain = setup_rag(vector_store, model, chunks)\n",
    "\n",
    "    # Cargar historial previo\n",
    "    chat_history = load_chat_history()\n",
    "    if chat_history:\n",
    "        print(\"\\nHistorial de conversaci√≥n cargado:\")\n",
    "\n",
    "    print(\"\\n¬°Asistente virtual para Bajaj Boxer CT100 KS listo!\")\n",
    "    print(\"Di tu pregunta (o 'salir' para terminar):\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"ingresa tu pregunta: \") #query_voz() \n",
    "        if not query:\n",
    "            continue\n",
    "        if query.lower() == \"salir\":\n",
    "            break\n",
    "\n",
    "        result = rag_chain.invoke({\"question\": query})\n",
    "        respuesta = result[\"answer\"]\n",
    "        print(\"\\nRespuesta ü§ñ:\", respuesta)\n",
    "        decir_respuesta(respuesta)\n",
    "\n",
    "        # Guardar en historial\n",
    "        chat_history.append({\"question\": query, \"answer\": respuesta})\n",
    "        save_chat_history(chat_history)\n",
    "\n",
    "        # Guardar para evaluaci√≥n con RAGAS\n",
    "        # El contexto recuperado est√° en result[\"source_documents\"]\n",
    "        contexts = \"\\n\\n\".join([doc.page_content for doc in result.get(\"source_documents\", [])])\n",
    "        save_ragas_history(query, respuesta, contexts)\n",
    "\n",
    "        # Preguntar si desea hacer otra consulta o salir\n",
    "        opcion = input(\"\\n¬øQuieres hacer otra consulta? (S para salir, cualquier otra tecla para hacer una nueva pregunta): \").strip().lower()\n",
    "        if opcion == \"s\":\n",
    "            break\n",
    "        limpiar_consola()\n",
    "\n",
    "    print(\"¬°Gracias por usar el asistente!\")\n",
    "    limpiar_consola()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d3abdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øQuieres (C)argar la base vectorial existente o (R)ecrearla con los documentos actuales?\n",
      "Cargando base de datos vectorial...\n",
      "Configurando sistema RAG con memoria...\n",
      "\n",
      "Historial de conversaci√≥n cargado:\n",
      "\n",
      "¬°Asistente virtual para Bajaj Boxer CT100 KS listo!\n",
      "Di tu pregunta (o 'salir' para terminar):\n",
      "\n",
      "Respuesta ü§ñ: ¬°Hola! Para tu Bajaj Boxer CT100 KS, la presi√≥n recomendada para las llantas es la siguiente:\n",
      "\n",
      "*   **Trasera (Solo):** 28 psi\n",
      "*   **Trasera (Con pasajero):** 32 psi\n",
      "\n",
      "Recuerda verificar regularmente el indicador de desgaste de las llantas para asegurar una conducci√≥n segura. ¬°Espero que esta informaci√≥n te sea √∫til!\n",
      "¬°Gracias por usar el asistente!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)  # Para que no salgan los warnings de pdfplumber\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
